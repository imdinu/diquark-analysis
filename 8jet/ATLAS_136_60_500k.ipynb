{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from diquark.constants import DATA_KEYS, PATH_DICT_ATLAS_136_60_500K, NEW_CROSS_SECTION_ATLAS_136_60\n",
    "from diquark.helpers import create_data_dict\n",
    "from diquark.load import read_jet_delphes, lower_cut_suu_mass\n",
    "from diquark.features import (\n",
    "    jet_multiplicity,\n",
    "    jet_btag_multiplicity,\n",
    "    leading_jet_arr,\n",
    "    calculate_delta_r,\n",
    "    combined_invariant_mass,\n",
    "    n_jet_invariant_mass,\n",
    "    n_jet_vector_sum_pt,\n",
    ")\n",
    "from diquark.plotting import make_histogram, make_histogram_with_double_gaussian_fit\n",
    "\n",
    "\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(key, path):\n",
    "    # Read the dataset\n",
    "    if not key.startswith(\"SIG\"):\n",
    "        data = read_jet_delphes(path)\n",
    "    else:\n",
    "        data = lower_cut_suu_mass(read_jet_delphes(path), 6000)\n",
    "\n",
    "    # Process the dataset\n",
    "    result = {}\n",
    "    result[\"multiplicity\"] = jet_multiplicity(data)\n",
    "    result[\"btag_multiplicity\"] = jet_btag_multiplicity(data)\n",
    "\n",
    "    result[\"pt\"] = leading_jet_arr(data, key=\"Jet/Jet.PT\", n=8)\n",
    "    result[\"eta\"] = leading_jet_arr(data, key=\"Jet/Jet.Eta\", n=8)\n",
    "    result[\"phi\"] = leading_jet_arr(data, key=\"Jet/Jet.Phi\", n=8)\n",
    "\n",
    "    result[\"inv_mass\"] = combined_invariant_mass(data, n=8)\n",
    "\n",
    "    # Calculate ΔR for each pair of jets\n",
    "    delta_rs = calculate_delta_r(result[\"eta\"], result[\"phi\"], result[\"pt\"])\n",
    "    result[\"delta_R\"] = delta_rs\n",
    "    result[\"max_delta_R\"] = np.max(delta_rs, axis=1)\n",
    "\n",
    "    # Calculate invariant masses for 3-jet and 2-jet combinations\n",
    "    m3j_s = n_jet_invariant_mass(data, n=8, k=3)\n",
    "    m2j_s = n_jet_invariant_mass(data, n=8, k=2)\n",
    "    result[\"m3j\"] = m3j_s\n",
    "    result[\"m2j_s\"] = m2j_s\n",
    "\n",
    "    result[\"m3j_m8j\"] = np.divide(\n",
    "        m3j_s.mean(axis=-1, where=m3j_s != 0),\n",
    "        result[\"inv_mass\"],\n",
    "        out=np.zeros_like(result[\"inv_mass\"]),\n",
    "        where=result[\"inv_mass\"] != 0,\n",
    "    )\n",
    "    result[\"m2j_m8j\"] = np.divide(\n",
    "        m2j_s.mean(axis=-1, where=m2j_s != 0),\n",
    "        result[\"inv_mass\"],\n",
    "        out=np.zeros_like(result[\"inv_mass\"]),\n",
    "        where=result[\"inv_mass\"] != 0,\n",
    "    )\n",
    "\n",
    "    # Find the mass of the jet pair with the smallest ΔR\n",
    "    smallest_delta_r_indices = np.argmin(delta_rs, axis=1)\n",
    "    result[\"m2j_min_delta_R\"] = np.choose(smallest_delta_r_indices, m2j_s.T)\n",
    "\n",
    "    # Count jet pairs within 20 GeV of the W mass\n",
    "    result[\"nj_mW_pm20\"] = np.sum((m2j_s >= 60) & (m2j_s <= 100), axis=1)\n",
    "\n",
    "    # Calculate vector sum pT for 2-jet combinations\n",
    "    vector_sum_pts = n_jet_vector_sum_pt(data, n=8, k=2)\n",
    "    result[\"max_vector_sum_pt\"] = np.max(vector_sum_pts, axis=1)\n",
    "\n",
    "    # Indices of the jet pairs (flat index across the combination matrix)\n",
    "    jet_pair_indices = np.argmax(vector_sum_pts, axis=1)\n",
    "\n",
    "    # calculate the ΔR between the two jets with the largest vector sum pT\n",
    "    result[\"max_vector_sum_pt_delta_r\"] = np.choose(jet_pair_indices, delta_rs.T)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets one at a time\n",
    "results = {}\n",
    "for key in tqdm(DATA_KEYS):\n",
    "    results[key] = process_dataset(key, PATH_DICT_ATLAS_136_60_500K[key])\n",
    "\n",
    "# Create the final DataFrame\n",
    "df_list = []\n",
    "for key, data in results.items():\n",
    "    row = pd.DataFrame({feature: [data[feature]] for feature in data.keys()})\n",
    "    row[\"Truth\"] = key\n",
    "    row[\"target\"] = 1 if \"SIG\" in key else 0\n",
    "    df_list.append(row)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pendulum\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "import diquark.constants as const\n",
    "from diquark.plotting import make_histogram, make_histogram_with_double_gaussian_fit\n",
    "from diquark.helpers import mass_score_cut\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = pendulum.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "workdir = f\"run_ATLAS_136_60_{run_id}\"\n",
    "\n",
    "if not os.path.exists(workdir):\n",
    "    os.makedirs(workdir)\n",
    "    os.makedirs(f\"{workdir}/data\")\n",
    "    os.makedirs(f\"{workdir}/plots\")\n",
    "\n",
    "df.to_parquet(f\"{workdir}/data/full_sample.parquet\", index=False)\n",
    "print(f\"Saving models to {workdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"{workdir}/data/full_sample.parquet\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate signal and background events\n",
    "df_bkg = df[df[\"target\"] == 0]\n",
    "df_sig = df[df[\"target\"] == 1]\n",
    "\n",
    "# Split the background events\n",
    "df_bkg_train = df_bkg.sample(frac=0.8, random_state=0)\n",
    "df_bkg_test = df_bkg.drop(df_bkg_train.index)\n",
    "\n",
    "# Separate the signal events\n",
    "df_sig_test = df_sig.sample(n=len(df_bkg_test) // df_bkg[\"Truth\"].nunique(), random_state=0)\n",
    "df_sig_train = df_sig.drop(df_sig_test.index)\n",
    "\n",
    "\n",
    "# Oversample the signal class in the training set to match the number of background instances\n",
    "df_sig_train_oversampled = resample(\n",
    "    df_sig_train,\n",
    "    replace=True,  # sample with replacement\n",
    "    n_samples=len(df_bkg_train),  # match number in majority class\n",
    "    random_state=0,\n",
    ")  # reproducible results\n",
    "\n",
    "# Combine the oversampled signal class with the background class to form the training set\n",
    "df_train = pd.concat([df_sig_train_oversampled, df_bkg_train])\n",
    "\n",
    "# Combine signal and background test sets\n",
    "df_test = pd.concat([df_sig_test, df_bkg_test])\n",
    "\n",
    "# Shuffle the training and test sets\n",
    "df_train = shuffle(df_train, random_state=0)\n",
    "df_test = shuffle(df_test, random_state=0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Separate features and targets\n",
    "x_train = df_train.drop([\"target\", \"Truth\", \"inv_mass\"], axis=1).to_numpy()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = df_test.drop([\"target\", \"Truth\", \"inv_mass\"], axis=1).to_numpy()\n",
    "x_test = scaler.transform(x_test)\n",
    "y_train = df_train[\"target\"].to_numpy()\n",
    "y_test = df_test[\"target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(f\"{workdir}/data/train.parquet\")\n",
    "df_test.to_parquet(f\"{workdir}/data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_test[[\"Truth\", \"inv_mass\"]].reset_index(drop=True)\n",
    "train_df = df_train[[\"Truth\", \"inv_mass\"]].reset_index(drop=True)\n",
    "\n",
    "m8j_test = {}\n",
    "for key in test_df[\"Truth\"].unique():\n",
    "    m8j_test[key] = test_df[test_df[\"Truth\"] == key][\"inv_mass\"].to_numpy()\n",
    "\n",
    "m8j_train = {}\n",
    "for key in train_df[\"Truth\"].unique():\n",
    "    m8j_train[key] = train_df[train_df[\"Truth\"] == key][\"inv_mass\"].to_numpy()\n",
    "\n",
    "joblib.dump(m8j_test, f\"{workdir}/data/m8j_test.data.joblib\")\n",
    "joblib.dump(m8j_train, f\"{workdir}/data/m8j_train.data.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f\"{workdir}/data/data.npz\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_instance = tfk.metrics.FalsePositives(name=\"fp\")\n",
    "\n",
    "model = tfk.Sequential(\n",
    "    [\n",
    "        tfkl.InputLayer(input_shape=(145)),\n",
    "        tfkl.Dense(64, activation=\"relu\", name=\"dense_1\"),\n",
    "        tfkl.Dropout(0.2, name=\"dropout_1\"),\n",
    "        tfkl.Dense(32, activation=\"relu\", name=\"dense_2\"),\n",
    "        tfkl.Dropout(0.1, name=\"dropout_2\"),\n",
    "        tfkl.Dense(32, activation=\"relu\", name=\"dense_3\"),\n",
    "        tfkl.Dense(1, activation=\"sigmoid\", name=\"output\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[metric_instance, \"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=history.epoch,\n",
    "        y=history.history[\"loss\"],\n",
    "        name=\"Training Loss\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=history.epoch,\n",
    "        y=history.history[\"val_loss\"],\n",
    "        name=\"Validation Loss\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Training and Validation Loss\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Loss\",\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{workdir}/data/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = model.predict(x_test)\n",
    "scores_test_nn = {}\n",
    "for key in test_df[\"Truth\"].unique():\n",
    "    scores_test_nn[key] = y_pred_nn.flatten()[test_df[test_df[\"Truth\"] == key].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_histogram(scores_test_nn, 50, clip_top_prc=100, clip_bottom_prc=0, cross=None)\n",
    "fig.update_layout(\n",
    "    title_text=\"Test scores distribution\",\n",
    "    barmode=\"stack\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"Random Forest Output\",\n",
    "    yaxis_title=\"Count\",\n",
    ")\n",
    "fig.write_image(f\"{workdir}/plots/NN-output.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _, _ = make_histogram_with_double_gaussian_fit(\n",
    "    mass_score_cut(m8j_test, scores_test_nn, cut=0.99, prc=True),\n",
    "    20,\n",
    "    clip_top_prc=100,\n",
    "    cross=NEW_CROSS_SECTION_ATLAS_136_60,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"8-jet Mass\",\n",
    "    xaxis_title=\"Invariant Mass [GeV]\",\n",
    "    yaxis_title_text=\"count x sigma\",\n",
    "    barmode=\"stack\",\n",
    "    bargap=0,\n",
    "    width=1600 * (5 / 6),\n",
    "    height=900 * (5 / 6),\n",
    ")\n",
    "fig.write_image(f\"{workdir}/plots/8jet_mass_NN_cut_fit.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=-1).fit(x_train, y_train)\n",
    "print(\"RFC trained\")\n",
    "joblib.dump(rf_clf, f\"{workdir}/data/rfc.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = xgb.XGBClassifier(tree_method=\"hist\").fit(x_train, y_train)\n",
    "print(\"GBC trained\")\n",
    "joblib.dump(gb_clf, f\"{workdir}/data/gbc.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = gb_clf.predict_proba(x_test)[:, 1]\n",
    "y_pred_rf = rf_clf.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = [NEW_CROSS_SECTION_ATLAS_136_60[label] for label in test_df[\"Truth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first model\n",
    "# precision_nn, recall_nn, thresholds_nn = precision_recall_curve(y_test, y_pred_nn)\n",
    "precision_nn, recall_nn, thresholds_nn = precision_recall_curve(\n",
    "    y_test, y_pred_nn, sample_weight=sample_weights\n",
    ")\n",
    "pr_auc_nn = auc(recall_nn, precision_nn)\n",
    "\n",
    "# For the Gradient Boosting model\n",
    "# precision_gb, recall_gb, thresholds_gb = precision_recall_curve(y_test, y_pred_gb)\n",
    "precision_gb, recall_gb, thresholds_gb = precision_recall_curve(\n",
    "    y_test, y_pred_gb, sample_weight=sample_weights\n",
    ")\n",
    "pr_auc_gb = auc(recall_gb, precision_gb)\n",
    "\n",
    "# For the Random Forest model\n",
    "# precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, y_pred_rf)\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n",
    "    y_test, y_pred_rf, sample_weight=sample_weights\n",
    ")\n",
    "pr_auc_rf = auc(recall_rf, precision_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sig) / (len(df_bkg) // df_bkg[\"Truth\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall_gb,\n",
    "        y=precision_gb,\n",
    "        customdata=thresholds_gb,\n",
    "        hovertemplate=\"Threshold=%{customdata}<br>Recall=%{x}<br>Precision=%{y}\",\n",
    "        mode=\"lines\",\n",
    "        name=f\"BDT - AUC={pr_auc_gb:.3f}\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall_rf,\n",
    "        y=precision_rf,\n",
    "        customdata=thresholds_rf,\n",
    "        hovertemplate=\"Threshold=%{customdata}<br>Recall=%{x}<br>Precision=%{y}\",\n",
    "        mode=\"lines\",\n",
    "        name=f\"RF - AUC={pr_auc_rf:.3f}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall_nn,\n",
    "        y=precision_nn,\n",
    "        customdata=thresholds_nn,\n",
    "        hovertemplate=\"Threshold=%{customdata}<br>Recall=%{x}<br>Precision=%{y}\",\n",
    "        mode=\"lines\",\n",
    "        name=f\"NN - AUC={pr_auc_nn:.3f}\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Cross-Section Weighted Precision-Recall Curves\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\",\n",
    "    width=1200 * (2 / 3),\n",
    "    height=800 * (2 / 3),\n",
    ")\n",
    "fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1))\n",
    "fig.write_image(f\"{workdir}/plots/PR-curve.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "rf_importance = rf_clf.feature_importances_\n",
    "gb_importance = gb_clf.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = df_train.drop([\"target\", \"Truth\", \"inv_mass\"], axis=1).columns\n",
    "\n",
    "# Create a grouped bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for Random Forest\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=feature_names,\n",
    "        y=rf_importance,\n",
    "        name=\"Random Forest\",\n",
    "        offsetgroup=1,\n",
    "        marker=dict(color=\"#E4D91B\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add bars for Random Forest\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=feature_names,\n",
    "        y=gb_importance,\n",
    "        name=\"Gradient Boosting\",\n",
    "        offsetgroup=2,\n",
    "        marker=dict(color=\"#D91BE4\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Importances for Random Forest\",\n",
    "    xaxis_title=\"Features\",\n",
    "    yaxis_title=\"Importance Value\",\n",
    "    # legend_title='Classifier',\n",
    "    xaxis=dict(tickangle=45),\n",
    "    barmode=\"group\",\n",
    "    width=1200,\n",
    ")\n",
    "fig.write_image(f\"{workdir}/plots/feature_importances.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_rf = {}\n",
    "for key in test_df[\"Truth\"].unique():\n",
    "    scores_test_rf[key] = y_pred_rf.flatten()[test_df[test_df[\"Truth\"] == key].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_histogram(scores_test_rf, 50, clip_top_prc=100, clip_bottom_prc=0, cross=None)\n",
    "fig.update_layout(\n",
    "    title_text=\"Data Sample Content by Model Output Cut\",\n",
    "    barmode=\"stack\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"RF Output\",\n",
    "    yaxis_title=\"Probability Density\",\n",
    "    width=1600 * (5 / 6),\n",
    "    height=900 * (5 / 6),\n",
    ")\n",
    "fig.write_image(f\"{workdir}/plots/RF-output.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 features by importance\n",
    "print(\"Top 10 features by importance\")\n",
    "print(\"Random Forest\")\n",
    "print(feature_names[np.argsort(rf_importance)[::-1][:10]])\n",
    "print(\"Gradient Boosting\")\n",
    "print(feature_names[np.argsort(gb_importance)[::-1][:10]])\n",
    "\n",
    "# vertical bar chart for random forest top 10\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        y=feature_names[np.argsort(rf_importance)[::-1][:10]][::-1],\n",
    "        x=rf_importance[np.argsort(rf_importance)[::-1][:10]][::-1],\n",
    "        name=\"Random Forest\",\n",
    "        offsetgroup=1,\n",
    "        marker=dict(color=\"#E4D91B\"),\n",
    "        orientation=\"h\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Top 10 Features by Importance for Random Forest\",\n",
    "    yaxis_title=\"Features\",\n",
    "    xaxis_title=\"Importance Value\",\n",
    "    # legend_title='Classifier',\n",
    "    xaxis=dict(tickangle=45),\n",
    "    barmode=\"group\",\n",
    "    width=600,\n",
    "    height=800,\n",
    ")\n",
    "fig.write_image(f\"{workdir}/plots/top10_RF.pdf\")\n",
    "fig.write_image(f\"{workdir}/plots/top10_RF.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rf = {}\n",
    "for cut in (0.2, 0.5, 0.8, 0.90, 0.925, 0.95, 0.96, 0.97, 0.98, 0.99):\n",
    "    scores = mass_score_cut(m8j_test, scores_test_rf, cut, prc=True)\n",
    "    counts = {\n",
    "        k: len(v)\n",
    "        * const.ATLAS_TOTAL_LUMI\n",
    "        * NEW_CROSS_SECTION_ATLAS_136_60[k]\n",
    "        / m8j_test[key].shape[0]\n",
    "        for k, v in scores.items()\n",
    "    }\n",
    "    res_rf[cut] = counts\n",
    "df_counts_rf = pd.DataFrame(res_rf)\n",
    "# move index SIG:Suu to the bottom\n",
    "sig_row = df_counts_rf.loc[\"SIG:Suu\"]\n",
    "df_counts_rf = df_counts_rf.drop(\"SIG:Suu\")\n",
    "df_counts_rf = pd.concat([df_counts_rf, sig_row.to_frame().T])\n",
    "df_counts_rf\n",
    "\n",
    "bkg_counts_rf = df_counts_rf.iloc[:-1].T.sum(axis=1)\n",
    "sig_counts_rf = df_counts_rf.iloc[-1]\n",
    "s_over_b_rf = sig_counts_rf / bkg_counts_rf\n",
    "\n",
    "df_counts_rf.loc[\"BKG:sum\"] = bkg_counts_rf\n",
    "df_counts_rf.loc[\"S/B\"] = s_over_b_rf\n",
    "df_counts_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_counts = df_counts_rf.iloc[:-1].T.sum(axis=1)\n",
    "sig_counts = df_counts_rf.iloc[-1]\n",
    "s_over_sb = sig_counts / np.sqrt(bkg_counts)\n",
    "s_over_b = sig_counts / bkg_counts\n",
    "s_over_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_rf.to_csv(f\"{workdir}/data/counts_rf.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
